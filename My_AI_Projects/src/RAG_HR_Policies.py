'''
RAG Exercise with Gradio Interface

Steps:
1. Upload PDF file - HR Policy Manual from some organization
2. Split into chunks
3. Create vector store
4. Create retrieval QA chain
5. Create Gradio interface
'''
import os, sys
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

import gradio as gr

project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from global_util.gopi_util import get_llm, create_or_load_chroma_db


# ================Upload PDF file and Split into chunks===============
'''
*********** Replace the below file path with your own HR Policy Manual PDF file ***********
'''

pdf_path = "./global_test_files/HR-Manual.pdf" # Relative path to HR Policy Manual PDF file

loader = PyPDFLoader(pdf_path)
documents = loader.load()

# ================Split into chunks===============
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
docs = splitter.split_documents(documents)

# ================Create Vector Store===============
vector_store = create_or_load_chroma_db(docs, db_name="hr_policy_chroma_db", collection_name="hr_policy_pdf")

# ================Create Retrieval QA Chain===============
retriever = vector_store.as_retriever()

# ================Create Prompt Template===============
# This template guides the LLM on how to use the retrieved context from the vector store to answer questions
# {context} - is the retrieved data from vector store and {question} - is the user query that needs to be answered
# Answer - is the placeholder for the final response that will be generated by the LLM
# The template is used to format the prompt before sending it to the LLM
#  ================End of Prompt Template===============

template = """You are an HR assistant. Use the following pieces of retrieved context to answer the question. 
If you don't know the answer from the context, just say that you don't know. Keep the answer concise.

Context: {context}

Question: {question}

Answer:
"""

prompt = ChatPromptTemplate.from_template(template)

# ================Create QA Chain===============
# This chain retrieves relevant documents from the vector store and uses them to answer the user's question
# The chain is composed of the retriever, prompt, and LLM
#  ================End of QA Chain===============
qa_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | get_llm()
    | StrOutputParser()
)

# ================Create Gradio Interface===============
# This interface allows users to interact with the HR assistant
# Users can ask questions about HR policies and get answers based on the retrieved context
# This will run as a web application and open in a browser
#  ================End of Gradio Interface===============
def chatbot(user_input):
    try:
        return qa_chain.invoke(user_input)
    except Exception as e:
        return str(e)

demo = gr.Interface(
    fn=chatbot, 
    inputs=gr.Textbox(lines=3, label="Ask HR Assistant:", placeholder="Enter your HR question here..."), 
    outputs=gr.Textbox(label="Response:", lines=12), 
    title="AI powered HR Assistant",
    description="Ask HR Assistant any question about HR policy"
)

demo.launch(share=True) # Share the interface with the world



